###### 怎么解决模型的输出结尾出现重复字符的问题：
> 模型的训练数据一般是正向问题，尽量不要出现否定的语句。

###### 为什么大模型难以使用长文本
简而言之，self-attention 使同一上下文的每一个token都能够观察到上下文得任意位置的 token（decoder 的 self-attention 是每个token只能观察到上下文任意历史位置的token），这一结构特点使得Transformer 较之CNN、RNN等模型结构理论上显著提升了长距离依赖的捕捉能力，无数实验也证实了这种结构可以提升最终的效果。付出的代价就是，与此同时计算复杂度也增长为平方级。计算时间和计算资源的制约是 n 无法迅速增大的直接因素，也就是在平方级复杂度制约下，模型上下文难以随心所欲地增长。
![image](https://github.com/Feve1986/coding/assets/67903547/bfcd1e9a-6385-4f05-8fc4-8bf96c1d5945)

###### 解决方案
* 1.借助模型外部工具辅助处理长文本或者利用外部记忆存储过长的上下文向量，可以称为外部召回的方案；
* 2.利用模型优化的一般方法：包括量化（Quantization）、剪枝（Pruning）、蒸馏（Distillation）、参数共享（Weight Sharing）、矩阵分解（Factorization）
* 3.优化Attention的计算（Efficient Transformers）：
  Transformer-XL(相对位置编码), Sparse Patterns，Low-Rank Transformation，Memory / Downsampling
  
###### 上下文长度越长越好吗
* 上下文过长会导致注意力分散。
  
###### kimi
通过创新的网络结构和工程优化，克服大模型在长文本处理上的挑战，不依赖于滑动窗口、降采样、小模型等性能性能损失较大的方案，实现了支持20w字输入的200b参数大模型kimi。研发更长上下文长度的模型，以覆盖更多场景并提高应用效果。
kimi的优势：
* 长文本处理能力：支持20w字输入。
* 无损记忆能力：在处理长文本信息时保持信息的完整性和连贯性。
* 高效的长程注意力机制：不依赖于滑动窗口、降采样、小模型等简化处理方法。
* 多语言能力
  
###### Baichuan-192k
上下文窗口长度支持192k token，约等于35w汉字。实现技术：
* 算法层面：提出一种基于RoPE和ALiBi的动态位置编码的外推方案，对不同分辨率的ALiBi_mask进行不同程度的Attention-mask动态内插，
  在保证分辨率的同时增强模型对长序列依赖的建模能力。
* 工程层面：在自主开发的分布式训练框架上，整合优化技术如张量并行、流水并行、序列并行、重计算和Offload功能

###### InfLLM（支持1024k输入）
1. 在滑动窗口的基础上，加入远距离的上下文记忆模块。  
2. 将历史上下文切分成语义块，构成上下文记忆模块中的记忆单元。每个记忆单元通过其在之前注意力计算中的注意力分数确定代表性Token，作为记忆单元的表示。从而避免上下文中的噪音干扰，并降低记忆查询复杂度  
