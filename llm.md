###### 怎么解决模型的输出结尾出现重复字符的问题：
> 模型的训练数据一般是正向问题，尽量不要出现否定的语句。

###### 为什么大模型难以使用长文本
简而言之，self-attention 使同一上下文的每一个token都能够观察到上下文得任意位置的 token（decoder 的 self-attention 是每个token只能观察到上下文任意历史位置的token），这一结构特点使得Transformer 较之CNN、RNN等模型结构理论上显著提升了长距离依赖的捕捉能力，无数实验也证实了这种结构可以提升最终的效果。付出的代价就是，与此同时计算复杂度也增长为平方级。计算时间和计算资源的制约是 n 无法迅速增大的直接因素，也就是在平方级复杂度制约下，模型上下文难以随心所欲地增长。
![image](https://github.com/Feve1986/coding/assets/67903547/bfcd1e9a-6385-4f05-8fc4-8bf96c1d5945)

###### 长上下文的解决方案
1.借助模型外部工具辅助处理长文本或者利用外部记忆存储过长的上下文向量，可以称为外部召回的方案；
2.利用模型优化的一般方法：包括量化（Quantization）、剪枝（Pruning）、蒸馏（Distillation）、参数共享（Weight Sharing）、矩阵分解（Factorization）
3.优化Attention的计算（Efficient Transformers）：
  Transformer-XL(相对位置编码), Sparse Patterns，Low-Rank Transformation，Memory / Downsampling
4. RopE的位置编码扩展
5. 

###### 流水线并行

###### 上下文长度越长越好吗
* 上下文过长会导致注意力分散。
  
###### kimi
通过创新的网络结构和工程优化，克服大模型在长文本处理上的挑战，不依赖于滑动窗口、降采样、小模型等性能性能损失较大的方案，实现了支持20w字输入的200b参数大模型kimi。研发更长上下文长度的模型，以覆盖更多场景并提高应用效果。
kimi的优势：
* 长文本处理能力：支持20w字输入。
* 无损记忆能力：在处理长文本信息时保持信息的完整性和连贯性。
* 高效的长程注意力机制：不依赖于滑动窗口、降采样、小模型等简化处理方法。
* 多语言能力
  
###### Baichuan-192k
上下文窗口长度支持192k token，约等于35w汉字。实现技术：
* 算法层面：提出一种基于RoPE和ALiBi的动态位置编码的外推方案，对不同分辨率的ALiBi_mask进行不同程度的Attention-mask动态内插，
  在保证分辨率的同时增强模型对长序列依赖的建模能力。
* 工程层面：在自主开发的分布式训练框架上，整合优化技术如张量并行、流水并行、序列并行、重计算和Offload功能
* Bfloat16混合精度训练；采用了NormHead，对输出embedding进行归一化处理

###### InfLLM（支持1024k输入）
1. 在滑动窗口的基础上，加入远距离的上下文记忆模块。  
2. 将历史上下文切分成语义块，构成上下文记忆模块中的记忆单元。每个记忆单元通过其在之前注意力计算中的注意力分数确定代表性Token，作为记忆单元的表示。从而避免上下文中的噪音干扰，并降低记忆查询复杂度

###### Gemini
Gemini 支持高达 1000万 token 的超长上下文和强大的多模态能力，这意味着利用 Gemini 能够与整本书籍、庞大的文档集、数百个文件组成的数十万行代码库、完整电影、整个播客系列等进行交互。

###### Claude：先监督微调 后RAIHF
RAIHF：通过AI排序而非人工排序数据集训练出来的偏好模型PM的指引下迭代模型策略

###### Llama2
相比于 Llama 1 ，Llama 2 的训练数据多了 40%，上下文长度也翻倍，并采用了分组查询注意力机制。具体来说，Llama 2预训练模型是在2 万亿的 token上训练的，精调 Chat 模型是在100 万人类标记数据上训练的。
![image](https://github.com/Feve1986/coding/assets/67903547/c51025fa-670b-425a-a2c1-e1ec3f283c69)
![image](https://github.com/Feve1986/coding/assets/67903547/fe751fe3-3b1d-4c97-9f1d-76b277e9bdcc)

###### DPO
RLHF的替代算法：直接偏好优化(Direct Preference Optimization，简称DPO)。DPO通过简单的分类目标直接优化最满足偏好的策略，而没有明确的奖励函数或RL。与RLHF一样，DPO依赖于理论偏好模型，衡量给定的奖励函数与经验偏好数据的一致性。

DPO利用从奖励函数到最优策略的解析映射，这使我们能够将奖励函数上的偏好损失函数转换为策略上的损失函数。具体做法是给定人类对模型响应的偏好数据集，DPO使用简单的二元交叉熵目标优化策略，而无需在训练期间明确学习奖励函数或从策略中采样。

###### Llama为什么使用RoPE
1. 序列长度灵活性、长度外推
2. 随着相对距离的增加而衰减的tokens间的依赖性，
3. 以及，为线性自注意力，配备（配装的）的相对位置编码的能力。
> RoPE对q和k做
![image](https://github.com/Feve1986/coding/assets/67903547/51d8ab07-fe1b-4664-9b41-60ee19fe4ca5)

长度外推的做法
![image](https://github.com/Feve1986/coding/assets/67903547/ad00967f-a786-4da9-bac4-e699586c5f55)


###### 子词分词器
* BPE：

1. BPE首先使用一个普通的分词器将语料切分成词，分词器可以选择前面提到的基于空格或基于规则的分词器。

2. 分完词后就得到了一个包含所有唯一单词的词表，并统计词表中每个单词出现的频次。

3. 创建一个包含了步骤2中的词表中所有符号的基础词汇表。

4. 从基础词汇表中选择两个符号根据合并规则形成一个新的符号，并更新基础词表。（BPE根据新符号出现的频率来合并的）

5. 重复第4步，直到基础词表的大小达到想要的大小。(词表的大小是一个预先定义的超参数)

* WordPiece:
1. 首先使用一个普通的分词器将语料切分成词，分词器可以选择前面提到的基于空格或基于规则的分词器。

2. 分完词后就得到了一个包含所有唯一单词的词表。

3. 创建一个包含了步骤2中的词表中所有符号的基础词汇表。

4. 从所有可能的组合中选择加入词表后能最大程度地增加语言模型在训练数据上的似然概率的组合

5. 重复第4步，直到基础词表的大小达到想要的大小。(词表的大小是一个预先定义的超参数)
![image](https://github.com/Feve1986/coding/assets/67903547/9384a79b-df58-467f-aff5-b20a9d5047de)

###### 激活函数
GELU是ReLU的平滑版本。

RELU的优缺点：
![image](https://github.com/Feve1986/coding/assets/67903547/954028f2-8ddf-48a7-b59a-7661c7f25c1c)

![image](https://github.com/Feve1986/coding/assets/67903547/51aecfc2-fd5d-44ab-824c-74a0cef3e54e)

###### jieba分词（中文分词框架）
![image](https://github.com/Feve1986/coding/assets/67903547/558e6836-5440-4ecd-8588-dea1bd1418c4)

###### 语言模型评价指标
困惑度（概率的高低，概率越高困惑度越低，不是最重要的指标）

###### 参数估计
* 最大似然估计：就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值。极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的。
  ![image](https://github.com/Feve1986/coding/assets/67903547/da9c52d4-28c7-426f-8f4c-f7933322ff1b)

 [DPO: Direct Preference Optimization 论文解读及代码实践] (https://zhuanlan.zhihu.com/p/642569664)

###### RAG
RAG 已经被证明是一种解决大模型幻觉的有效方法，如何进一步提升 RAG 的实战效果？

1.提升长上下文的理解能力

由于嵌入模型通常较小，上下文窗口有限，传统的 RAG 通常依赖分块对数据进行切分。这导致了上下文信息的丢失，例如一些代词的信息无法连贯地理解。

举例来说明，在某个对话中，提到 “Bill 周日去了埃菲尔铁塔，之后又跟朋友一起去了卢浮宫”。当我们进行传统的提问，例如询问“Bill周日下午去了哪里？”时，由于上下文信息被分割成多个分块，可能会导致搜索到的信息仅包含了Bill周日去了埃菲尔铁塔，从而形成了错误的结论。这种情况下，由于上下文被切分，系统无法正确理解代词“去了哪里”的指代对象，从而导致了错误的结果。

近期，基于大型模型实现的嵌入逐渐成为主流。在 Huggingface MTEB LeaderBoard 中，效果最好的嵌入基本上都是由大型模型所霸榜。这一趋势的一个副产品是嵌入的上下文窗口也逐渐提升，例如 SRF-Embedding-Mistral 和 GritLM7B 已经支持 32k 的长上下文，这意味着嵌入本身处理长上下文的能力也得到了大幅提升。

最近发布的 BGE Landmark embedding 的论文也阐述了一种利用长上下文解决信息不完整检索的方法。通过引入无分块的检索方法，Landmark embedding 能够更好地保证上下文的连贯性，并通过在训练时引入位置感知函数来有限感知连续信息段中最后一个句子，保证嵌入依然具备与 Sentence Embedding 相近的细节。这种方法大幅提升了长上下文 RAG 的精度。

2.利用多路召回提升搜索质量

为了提升 RAG 的回复质量，关键在于能够检索到高质量的内容。数据清理、结构化信息提取以及多路混合查询，都是提高搜索质量的有效手段。最新的研究表明，相比稠密向量模型，使用如 Splade 这类稀疏向量模型，在域外知识搜索性能，关键词感知能力以及可解释方面表现更佳。最近开源的 BGE_M3 模型能够在同一模型中生成稀疏、稠密以及类似 Colbert 的 Token 多向量，通过不同类型的向量多路召回并结合大型模型进行排名，可以显著提高检索效果。这种混合查询的方法也被向量数据库厂商广泛接受，最近发布的 Milvus 2.4 版本也支持了稠密和稀疏向量的混合查询。

3.使用复杂策略提升RAG能力

开发大模型应用不仅面临算法挑战，还涉及复杂的工程问题。这要求开发者具备深入的算法知识及复杂系统设计和工程实践的能力。采用复杂策略，如查询改写、意图识别和实体检测，不仅提升了准确性，也显著加快了处理速度。即使是先进的 Gemini 1.5 模型，在进行 Google 的 MMLU 基准测试时也需调用 32 次才能达到 90.0% 的准确率，显示出采用复杂工程策略以提升性能的必要性。通过使用向量数据库和 RAG，采取空间换时间的策略，使 RAG 系统能更有效利用大型语言模型（LLM）的能力。这不仅限于生成答案，还包括分类、提取结构化数据、处理复杂 PDF 文档等任务，增强了 RAG 系统的多功能性，使其能适应更广泛的应用场景。
![image](https://github.com/Feve1986/coding/assets/67903547/34cc63ef-3ad4-4463-b7ce-0a48d7afc7f3)
[RAG 修炼手册｜RAG 敲响丧钟？大模型长上下文是否意味着向量检索不再重要](https://segmentfault.com/a/1190000044755011)

###### Agent
* 什么是Agent: Model+Planning+Memory+Tools. 有工具，有记忆，能规划。
